---
title: "scratch paper"
author: "Lena Hicks"
date: "May 24, 2023"
output: 
  html_document:
    toc: true
    code_folding: hide 
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(here)
library(janitor)
library(ggeffects)
library(performance)
library(ggmice)
library(huxtable) 
library(broom)
library(corrplot)
library(GGally)
```

# Read in the data

```{r}
plant <- read_csv(here("data", "knb-lter-hfr.109.18","hf109-01-sarracenia.csv")) %>% 
  #make column names cleaner
  clean_names() %>% 
  #selecting the columns of interest
  select(totmass, species, feedlevel, sla, chlorophyll, amass, num_lvs, num_phylls)
```

# Visualize missing data

```{r visualize-missing-data}
plot_pattern(plant)
```

# Subsetting the data by dropping NAs:

```{r subset-drop-NA}
plant_subset <- plant %>% 
  drop_na(sla,chlorophyll, amass, num_lvs, num_phylls)
```

# Create a correlation plot:

(example writing) To determine the relationships between numerical variables in our dataset, we calculated Pearsons r and visually represented correlation using a correlation plot.

```{r correlation-plot}
#calculate pearsons r only for numeric values only
plant_cor <- plant_subset %>% 
  select(feedlevel:num_phylls) %>% 
  cor(method = "pearson")
  
#creating a correlation plot
corrplot(plant_cor,
         #change the shape of what's in the cells
         #angle of ellipse shows relationship: all point to right is postive, to left is negative
         method = "ellipse",
         addCoef.col = "black"
         )
```

# Create a plot fo each variable compared against the others

```{r pairs-plot}
plant_subset %>% 
  select(species:num_phylls) %>% 
  ggpairs()
```

# starting regression here:

(example) To determine how species and physiological characteristics predict biomass, we fit multiple linear models. 

```{r null-and-full-problem}
#to specifiy null model, use 1 as predictor (because no actual predictors, just intercept)
null <- lm(totmass ~ 1, data = plant_subset)
full <- lm(totmass ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data = plant_subset)
```


We visually assess normality and homoskedasticity of residuals using diagnostic plots for the full model:

```{r full-diagnostics}
par(mfrow = c(2,2))
plot(full)
```

Residuals vs Fitted plot: condensed at left end and spread on right - heterskedastic

We also tested for normality using the Shapiro-Wilk test (null hypothesis: variable of interest, ie.residuals, are normally distributed).

We tested for heterskedasticity using the Breusch-Pagan test (null hypothesis: variable of interest has constant variance (the resduals)).
```{r}
check_normality(full)
check_heteroscedasticity(full)
```


```{r model-logs}
null_log <- lm(log(totmass) ~ 1, data = plant_subset)
full_log <- lm(log(totmass) ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data = plant_subset)

plot(full_log)
check_normality(full_log)
check_heteroscedasticity(full_log)
```

Evaluate multicollinearity:

```{r calculate-vif}
#library(car)
#car::vif(full_log)
```

If have a bunch of predictors in model related to eachother, adding more detail bumping up r2
VIF: Looking for combination of predictors that's adding more info to model but not actual needing it

"We evaluated multicollinearity by calculating gernalized variance inflation factor and determined that based on the gVIF no mulitcollinearity was not there."

try some more models:

addressing what set of predicotr variables best explains the response?

```{r}
model2_log <- lm(log(totmass) ~ species, data = plant_subset)
```

Check assumptions for model 2:
```{r}
par(mfrow = c(2,2))
plot(model2_log)

check_normality(model2_log)
check_heteroskedasticity(model2_log)
```
second model of only species as factor appears to follow linear regression based off checked assumptions

compare models using Akaike's Informatio criterion (AIC) values:
```{r}
AIC(full_log, model2_log, null_log)
```
Based on these AIC models, full model is best. 
In homework will be making 2 more models best chosen based on biology and comparing those to full and null

We compared models using AIC and chose the model with the lowest value which was the full model.

# Results

We found that the _____ model including ___ ____ ___ predictors best predicted ______ (model summary)
include pvalue, r2, table, etc.

```{r}
summary(full_log)

table <- tidy(full_log, conf.int = TRUE) %>% 
  #change p-value numbers if really small
  mutate(p.value = case_when(p.value < 0.001 ~ "<0.001")) %>% 
  #using huxtable as alternative to flextable 
  #change the estimates, standard error,etc.
  as_hux() 

#printing table  
table
```

HOMEWORK: no anova table needs to be made


  set_markdown_contents(1, 1, "Terms") %>% 
  set_markdown_contents(1, 2, "Degrees of Freedom") %>% 
  set_markdown_contents(1, 3, "Sum of Squares")%>% 
  set_markdown_contents(1,4, "Mean of Squares") %>% 
  set_markdown_contents(1,5, "F Statistic") %>% 
  set_markdown_contents(1,6, "P-Value") %>% 
  set_markdown_contents(2,1, "Length")





use ggpredict to backtransform estimates
```{r}
model_pred <- ggpredict(full_log, terms = "species", back.transform = TRUE)
plot(model_pred, add.data = TRUE)

plot(ggpredict(full_log, terms = "chlorophyll", back.transform = TRUE), add.data = TRUE)

plot(ggpredict(full_log, terms = "sla", back.transform = TRUE), add.data = TRUE)

model_pred
```
constant numbers are the "adjust for" parts


# different types of anova




